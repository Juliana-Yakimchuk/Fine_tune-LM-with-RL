{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce1ed17",
   "metadata": {},
   "source": [
    "### Table of Contents <a class=\"anchor\" id=\"part0\"></a>\n",
    "\n",
    "* [EDA](#part1)\n",
    "    * [Loading data](#section_1_1)\n",
    "    * [EDA](#section_1_2)\n",
    "* [Transfer Learning](#part2)  \n",
    "    * [](#section_2_1)\n",
    "    * [](#section_2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9f3d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3232efd",
   "metadata": {},
   "source": [
    "# EDA<a class=\"anchor\" id=\"part1\"></a>\n",
    "## Loading Data <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef372f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.38kB [00:00, 4.39MB/s]                   \n",
      "Downloading metadata: 2.83kB [00:00, 2.84MB/s]                   \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reddit/default (download: 2.93 GiB, generated: 17.64 GiB, post-processed: Unknown size, total: 20.57 GiB) to C:\\Users\\VR\\.cache\\huggingface\\datasets\\reddit\\default\\1.0.0\\98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.14G/3.14G [09:05<00:00, 5.76MB/s] \n"
     ]
    }
   ],
   "source": [
    "#dataset = load_dataset('reddit', download_mode= \"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a67d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f60a0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'NuffZetPand0ra',\n",
       " 'body': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :)\\n\\n\\nEDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though.\\n\\nAs far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P\\n\\nTLDR: Class only items dropped from high-lvl monsters.\",\n",
       " 'normalizedBody': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :) \\n EDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though. \\n As far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P \\n TLDR: Class only items dropped from high-lvl monsters. \\n\",\n",
       " 'subreddit': 'Diablo',\n",
       " 'subreddit_id': 't5_2qore',\n",
       " 'id': 'c6acxvc',\n",
       " 'content': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :) \\n EDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though. \\n As far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P\",\n",
       " 'summary': 'Class only items dropped from high-lvl monsters.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]\n",
    "#we only need the columns 'content' and 'summary' for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "351ca325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dataset.remove_columns(['author',\n",
    "  'body',\n",
    "  'normalizedBody',\n",
    "  'subreddit',\n",
    "  'subreddit_id',\n",
    "  'id'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c635a",
   "metadata": {},
   "source": [
    "### We'll shuffle the dataset and keep one million of examples. It's a very large dataset and we need to find  balance between efficient training and computational resources. In the paper \"RL4LM\" the authors used CNN daily dataset for fine-tuning an abstractive summarization model. The dataset contained 311K examples with 300K being train-validation and 11K being test. We'll keep 400K examples from our dataset for fine-tuning our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d7b632e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\VR\\.cache\\huggingface\\datasets\\reddit\\default\\1.0.0\\98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969\\cache-1bd07cb9828bf1ee.arrow\n"
     ]
    }
   ],
   "source": [
    "#first we'll shuffle dataset to ensure the randomness of examples\n",
    "shuffled_ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0802cf3",
   "metadata": {},
   "source": [
    "## EDA <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f0838bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to ensure that we get quality samples for the training. Lets first check the length of the posts and summaries\n",
    "num_tokens = []\n",
    "for post in (shuffled_ds['train']['content']):\n",
    "    words = post.split(' ')\n",
    "    num_tokens.append(len(words))\n",
    "num_tokens = sorted(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b1b9516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20964\n"
     ]
    }
   ],
   "source": [
    "print(num_tokens[0], num_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69689a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "print(num_tokens[int(len(num_tokens)/2)])\n",
    "#median post length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38267a3",
   "metadata": {},
   "source": [
    "### As we can see, the majorty of posts are under 2000 tokens/words long,  with a number of long outliers. There are also very short posts that do not contain enough information to create a summary.  For our dataset we will cap post length at 350 words to avoid computational overload, we will also remove any posts that are less than 20 words long, as we don't believe it would be sufficient for summary creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7aaa63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove posts that are too short or too long, or if their summary is longer than the post itself\n",
    "def len_content(post):\n",
    "    content = post['content'].split(' ')\n",
    "    summary = post['summary'].split(' ')\n",
    "    \n",
    "    if len(content)<=350 and len(content)>=20 and len(content)>len(summary):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6be88e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3849/3849 [02:02<00:00, 31.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'summary'],\n",
       "    num_rows: 2824498\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds = shuffled_ds['train'].filter(lambda x: len_content(x))\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "01689f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = []\n",
    "for post in (raw_ds['content']):\n",
    "    words = post.split(' ')\n",
    "    num_tokens.append(len(words))\n",
    "num_tokens = sorted(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5866651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 350\n"
     ]
    }
   ],
   "source": [
    "print(num_tokens[0], num_tokens[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bd1c266e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'summary'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will save the first 400K examples for our fine_tuning. The dataset has been preshuffled and only long enough posts\n",
    "prepped_ds = raw_ds.select(range(400000))\n",
    "prepped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4edd5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\VR\\.cache\\huggingface\\datasets\\reddit\\default\\1.0.0\\98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969\\cache-05f98df0e5f5a67d.arrow\n"
     ]
    }
   ],
   "source": [
    "#we'll save the raw dataset to our local computer to avoid reloading the full dataset from Huggingface\n",
    "prepped_ds.save_to_disk('C:/Users/VR/.cache/huggingface/datasets/prepped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39363e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'summary'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepped_ds = load_from_disk('C:/Users/VR/.cache/huggingface/datasets/prepped')\n",
    "prepped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ee1bcb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 360000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=prepped_ds.train_test_split(test_size = 0.1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f0129",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ce1ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cba9edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 754 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff48df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
