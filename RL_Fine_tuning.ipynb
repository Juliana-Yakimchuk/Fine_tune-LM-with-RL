{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62580bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676d9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe2cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef372f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.38kB [00:00, 4.39MB/s]                   \n",
      "Downloading metadata: 2.83kB [00:00, 2.84MB/s]                   \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset reddit/default (download: 2.93 GiB, generated: 17.64 GiB, post-processed: Unknown size, total: 20.57 GiB) to C:\\Users\\VR\\.cache\\huggingface\\datasets\\reddit\\default\\1.0.0\\98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.14G/3.14G [09:05<00:00, 5.76MB/s] \n"
     ]
    }
   ],
   "source": [
    "#dataset = load_dataset('reddit', download_mode= \"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a67d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ce69c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'NuffZetPand0ra',\n",
       " 'body': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :)\\n\\n\\nEDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though.\\n\\nAs far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P\\n\\nTLDR: Class only items dropped from high-lvl monsters.\",\n",
       " 'normalizedBody': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :) \\n EDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though. \\n As far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P \\n TLDR: Class only items dropped from high-lvl monsters. \\n\",\n",
       " 'subreddit': 'Diablo',\n",
       " 'subreddit_id': 't5_2qore',\n",
       " 'id': 'c6acxvc',\n",
       " 'content': \"You are talking about the Charsi imbue, right? Or a cube upgrade?\\nIf we are talking Charsi imbue, you can only imbue WHITE items. This includes superior, but they will not neccesarily be superior after imbuing (they get random base-modifications). Bloodfist and Gorefoot are both uniques (gold), and therefore not eligible for imbuing.\\nWhen you imbue, the item level matters (the item level is hidden). The item is the same level as the monster who dropped it. That means, that the higher level the monster who dropped it, the more stats is available on that item. It is important to note that an item doesn't neccesarily use all it's stat potential. This means that the same item dropped in a1 and a2 can has the possibility of some very different outcomes.\\nAfter the imbue, the item can be as good as if the monster itself had dropped a rare (yellow) item. Imbued weapons will always turn out as rare items.\\nTo answer your question, you should just progress like you are now, fighting the hardest monsters you can. When a potential good white item appears, try to imbue that. Class specifiq items has a better chance to give +skills to your class. Circlets has the higest bonuses regarding yellow items. And you can get an extra base stat advantage by using exceoptional (nightmare) items, which will drop in a4-5 from time to time. You can check out monster levels, and item qualities (normal, exceptional, elite) on arreat summit ofc. :) \\n EDIT (forgot infoz):\\nIn cases of imbue-eligible items with base bonuses (maces, wands, staffs and class specifique items, and any other item with +skills etc.) will loose their current bonuses due to the base item reroll. They can get extra skills from the base item AND the rarity class. Say, if you imbue a staff with +1 fireball, it will reroll that base staff. It might turn out with a staff with +1 icebolt and +1 warmth, and then you add the bonuses the rarity would give you, lets say +1 icebolt and +1 fireball. Then your +1 fireball staff will have turned into a +2 icebolt +1 fireball staff. It might as well turn into any other staff allowed by it's item lvl though. \\n As far as I recall, weapon damage and defense values are not rerolled. I am not 100% positive on this though. Haven't played d2 for a looong time :P\",\n",
       " 'summary': 'Class only items dropped from high-lvl monsters.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]\n",
    "#we only need the columns 'content' and 'summary' for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9589c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dataset.remove_columns(['author',\n",
    "  'body',\n",
    "  'normalizedBody',\n",
    "  'subreddit',\n",
    "  'subreddit_id',\n",
    "  'id'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586c65c",
   "metadata": {},
   "source": [
    "#we'll shuffle the dataset and keep one million of examples. It's a very large dataset and we need to find  balance between efficient training and computational resources. In the paper \"RL4LM\" the authors used CNN daily dataset for fine-tuning an abstractive summarization model. The dataset contained 311K examples with 300K being train-validation and 11K being test. We'll keep 400K examples from our dataset for fine-tuning our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e5be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\VR\\.cache\\huggingface\\datasets\\reddit\\default\\1.0.0\\98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969\\cache-1bd07cb9828bf1ee.arrow\n"
     ]
    }
   ],
   "source": [
    "#first we'll shuffle dataset to ensure the randomness of examples\n",
    "shuffled_ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de344bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='\\nThis corpus contains preprocessed posts from the Reddit dataset.\\nThe dataset consists of 3,848,330 posts with an average length of 270 words for content,\\nand 28 words for the summary.\\n\\nFeatures includes strings: author, body, normalizedBody, content, summary, subreddit, subreddit_id.\\nContent is used as document and summary is used as summary.\\n', citation='\\n@inproceedings{volske-etal-2017-tl,\\n    title = {TL;DR: Mining {R}eddit to Learn Automatic Summarization},\\n    author = {V{\"o}lske, Michael  and Potthast, Martin  and Syed, Shahbaz  and Stein, Benno},\\n    booktitle = {Proceedings of the Workshop on New Frontiers in Summarization},\\n    month = {sep},\\n    year = {2017},\\n    address = {Copenhagen, Denmark},\\n    publisher = {Association for Computational Linguistics},\\n    url = {https://www.aclweb.org/anthology/W17-4508},\\n    doi = {10.18653/v1/W17-4508},\\n    pages = {59--63},\\n    abstract = {Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a {``}TL;DR{\\'\\'} to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.},\\n}\\n', homepage='https://github.com/webis-de/webis-tldr-17-corpus', license='', features={'content': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='reddit', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=18936201253, num_examples=3848330, dataset_name='reddit')}, download_checksums={'https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1': {'num_bytes': 3141854161, 'checksum': 'c1a0f8c4374c7314d3c9ec50dd505303c536062d87037d4dca7035b89b36938a'}}, download_size=3141854161, post_processing_size=None, dataset_size=18936201253, size_in_bytes=22078055414)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53e8f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'summary'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds = shuffled_ds['train'].select(range(400000))\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c12d9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 400/400 [00:04<00:00, 90.41ba/s]\n"
     ]
    }
   ],
   "source": [
    "#we'll save the raw dataset to our local computer to avoid reloading the full dataset from Huggingface\n",
    "raw_ds.save_to_disk('C:/Users/VR/.cache/huggingface/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7664efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'summary'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds = load_from_disk('C:/Users/VR/.cache/huggingface/datasets')\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2005f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 360000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['content', 'summary'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds=raw_ds.train_test_split(test_size = 0.1)\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce1ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba9edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 663 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff48df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
